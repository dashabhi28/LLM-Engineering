{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "Technical Tutor \n",
    "A tool that takes a technical question,  and responds with an explanation. It is assumed that a local llm is running on your system and has been invoked with this code, for instance here llama 3.2 is engaged.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question and prompts\n",
    "question = \"\"\"\n",
    " Please explain what this code does and why:\n",
    " yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    " \"\"\"   # here is the question; type over this to ask something new\n",
    "system_prompt = \"You are a technical assistant that analyzes the question raised by the user and provides a specific technical response\"\n",
    "user_prompt = \"Please give a detailed, short and precise, explanation to the following: \" + question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd51e2ce-15ee-41bd-a274-0b99dbd8f60f",
   "metadata": {},
   "source": [
    "This code defines a Python function called messages. It takes one argument, question.\n",
    "\n",
    "def messages(question):: This line defines the function named messages and indicates that it accepts one parameter, question.\n",
    "return [...]: This line specifies what the function will return. In this case, it returns a list containing two dictionaries.\n",
    "{\"role\": \"system\", \"content\": system_prompt}: This is the first dictionary in the list. It represents a message from the \"system\" and its content is the value of the variable system_prompt. It's likely that system_prompt is a variable defined elsewhere that holds instructions or context for a language model.\n",
    "{\"role\": \"user\", \"content\": user_prompt}: This is the second dictionary in the list. It represents a message from the \"user\" and its content is the value of the variable user_prompt. Similar to system_prompt, user_prompt is probably a variable defined elsewhere that holds the user's input or query.\n",
    "In essence, the messages function creates a simple list of messages, typically used as input for a language model conversation. It includes a system message (based on system_prompt) and a user message (based on user_prompt). The question argument passed to the function doesn't seem to be used within the function's current definition. This might indicate that the function is intended to be a template for creating messages, and the question argument might be used in a more complex version of the function or in the definition of user_prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#messages function\n",
    "def messages(question):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f3275a-e46c-423a-bead-5f82186d4c1a",
   "metadata": {},
   "source": [
    "This code defines a Python function called answer that takes one argument, question.\n",
    "\n",
    "def answer(question):: This line defines the function named answer and indicates that it accepts one parameter, question, which is the user's input or query.\n",
    "response = openai.chat.completions.create(...): This line makes a call to the create method of the chat.completions object from the openai library. This is where the function interacts with a language model API.\n",
    "model = \"llama3.2\": Specifies the language model to use for generating the answer.\n",
    "messages = messages(question): Provides the list of messages for the conversation. It calls the messages function (which we discussed earlier) with the user's question to generate the system and user messages that will be sent to the language model.\n",
    "return response.choices[0].message.content: This line returns the content of the message from the language model's response. response.choices is a list of possible completions, and [0] selects the first (and usually the only) choice. .message.content extracts the actual text of the generated answer from the model.\n",
    "In essence, the answer function takes a user's question, formats it along with a system prompt using the messages function, sends this to the llama3.2 language model via the openai API, and returns the model's generated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the model and initiating response\n",
    "def answer(question):\n",
    "    response = openai.chat.completions.create(\n",
    "        model = \"llama3.2\",\n",
    "        messages = messages(question)\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc22a7da-267b-4ccf-80e9-8b09076a19dd",
   "metadata": {},
   "source": [
    "Print response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Code Explanation**\n",
      "\n",
      "The given code uses Python's `yield from` statement with a generator expression.\n",
      "\n",
      "```python\n",
      "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
      "```\n",
      "\n",
      "Here's what it does:\n",
      "\n",
      "1. Iterates over each `book` in the `books` collection.\n",
      "2. For each book, it checks if the book has an `\"author\"` key using `book.get(\"author\")`.\n",
      "3. If the book has an author, its value (i.e., the author's name) is **added to** an internal dictionary `{}` (i.e., created implicitly by the generator expression).\n",
      "4. The `yield from` statement then yields values from this internal dictionary, one author at a time.\n",
      "\n",
      "In essence, the code extracts authors from books and collects them in an internal dictionary, yielding each author separately.\n",
      "\n",
      "**Why**\n",
      "\n",
      "The use of `yield from` allows for efficient iteration over the extracted authors without loading the entire dictionary into memory. Instead, it generates each author value on-the-fly as needed, making this approach suitable for large datasets with many books.\n",
      "\n",
      "Note: The `{}` syntax is a shortcut to creating an empty dictionary in Python; it's equivalent to `dict()`.\n",
      "\n",
      "**Example Use Case**\n",
      "\n",
      "Suppose you have a list of book dictionaries:\n",
      "```python\n",
      "books = [\n",
      "    {\"id\": 1, \"title\": \"Book A\", \"author\": \"John Doe\"},\n",
      "    {\"id\": 2, \"title\": \"Book B\", \"author\": None},\n",
      "    {\"id\": 3, \"title\": \"Book C\", \"author\": \"Jane Smith\"}\n",
      "]\n",
      "\n",
      "authors = yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
      "print(authors)  # Output: {'John Doe', 'Jane Smith'}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "print(answer(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642a014-abef-47a2-b6be-13f2b106151a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
