{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "***Setting up the keys***\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e051a581-cb80-478a-8beb-b1cb2ea5970d",
   "metadata": {},
   "source": [
    "***Import libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04699b2-cf81-483b-9f2d-7be584ef26a1",
   "metadata": {},
   "source": [
    "#import google.generativeai #run if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4965864-b368-4f99-ab5e-11b61679446a",
   "metadata": {},
   "source": [
    "***Load environment***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key not set\n",
      "Anthropic API Key not set\n",
      "Google API Key not set\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1cd6f9-f47f-4c30-9a56-9f7cb29023fb",
   "metadata": {},
   "source": [
    "***Establish Connection***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ce3daf-50bd-47a5-8435-2cbe060f0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9abc522-0d7a-46aa-b07e-94482f691020",
   "metadata": {},
   "source": [
    "#google.generativeai.configure() #run if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "**temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic. 0 means more literal and 1 means creativity on steroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.chat.completions.create(\n",
    "    model='o3-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.7 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.7 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "***A rare problem with Claude streaming on Windows***\n",
    "\n",
    "Claude's streaming into Jupyter Lab's output -- it sometimes seems to eat up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc7567d-0b9d-4df4-9c6c-40cb44a32e19",
   "metadata": {},
   "source": [
    "***The API for Gemini***\n",
    "\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae07af-8c70-4f81-9159-832d0a8c4e8d",
   "metadata": {},
   "source": [
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash-preview-04-17\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7880f5c-e774-4e7a-a56d-a78a83dee361",
   "metadata": {},
   "source": [
    "***Deepseek***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ebbcb-4680-4a26-acee-3f71384bcb87",
   "metadata": {},
   "source": [
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9faf009-831b-4748-a1ec-14645b78ea4a",
   "metadata": {},
   "source": [
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2074066-5d4f-493a-86a4-1ca23c4b257c",
   "metadata": {},
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7487c89e-f297-4314-9a83-ffc56a930f21",
   "metadata": {},
   "source": [
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a33f9e4-53c4-4f25-a2eb-468420fe3210",
   "metadata": {},
   "source": [
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "***an adversarial conversation between Chatbots***\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efee3d4-c70e-4d08-92a6-a98db0653d57",
   "metadata": {},
   "source": [
    "This code defines a Python function called call_gpt.\n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "def call_gpt():: This line defines the function named call_gpt. It doesn't take any arguments, suggesting it might rely on variables defined outside the function's scope.\n",
    "messages = [{\"role\": \"system\", \"content\": gpt_system}]: Initializes a list called messages. The first item in the list is a dictionary representing a system message. The content of this message is taken from a variable named gpt_system, which is expected to be defined elsewhere and likely contains instructions for the GPT model.\n",
    "for gpt, claude in zip(gpt_messages, claude_messages):: This loop iterates through pairs of messages from two variables, gpt_messages and claude_messages. zip combines elements from both lists so that in each iteration, gpt will be an element from gpt_messages and claude will be an element from claude_messages. These variables are likely lists of strings or dictionaries representing previous turns in a conversation with two different models (presumably GPT and Claude).\n",
    "messages.append({\"role\": \"assistant\", \"content\": gpt}): Inside the loop, this line appends a dictionary to the messages list. This dictionary represents a message from the \"assistant\" (likely a previous response from the GPT model), with its content taken from the gpt variable in the current iteration.\n",
    "messages.append({\"role\": \"user\", \"content\": claude}): This line also appends a dictionary to the messages list. This dictionary represents a message from the \"user\" (likely a previous response from the Claude model or a user's input), with its content taken from the claude variable in the current iteration.\n",
    "completion = openai.chat.completions.create(...): This line makes a call to the create method of the chat.completions object from the openai library. This is where the function interacts with the GPT language model API.\n",
    "model=gpt_model: Specifies the GPT model to use for the completion. The model name is taken from the gpt_model variable, which is expected to be defined elsewhere.\n",
    "messages=messages: Provides the list of messages for the conversation history that was constructed in the previous steps.\n",
    "return completion.choices[0].message.content: This line returns the content of the message from the GPT model's response. completion.choices is a list of possible completions, and [0] selects the first (and usually the only) choice. .message.content extracts the actual text of the generated message from the model.\n",
    "In summary, the call_gpt function constructs a conversation history by interleaving system instructions and previous messages from two sources (gpt_messages and claude_messages), sends this history to a specified GPT model using the openai library, and returns the generated response from the GPT model. This pattern is often used in scenarios where you are orchestrating a conversation between multiple language models or incorporating previous turns into a new request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36751bb3-b323-4980-b88d-21308bc1e685",
   "metadata": {},
   "source": [
    "This code defines a Python function called call_claude.\n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "def call_claude():: This line defines the function named call_claude. Like call_gpt, it doesn't take any arguments directly, suggesting it relies on external variables.\n",
    "messages = []: Initializes an empty list called messages. This list will be populated with the conversation history for the Claude model.\n",
    "for gpt, claude_message in zip(gpt_messages, claude_messages):: This loop iterates through pairs of messages from gpt_messages and claude_messages using zip. In each iteration, gpt will be an element from gpt_messages and claude_message will be an element from claude_messages.\n",
    "messages.append({\"role\": \"user\", \"content\": gpt}): Inside the loop, this line appends a dictionary to the messages list. This dictionary represents a message from the \"user\", with its content taken from the gpt variable. This is interesting because it suggests that the responses from the GPT model (gpt_messages) are being treated as \"user\" input when constructing the conversation for the Claude model.\n",
    "messages.append({\"role\": \"assistant\", \"content\": claude_message}): This line appends a dictionary to the messages list. This dictionary represents a message from the \"assistant\" (likely a previous response from the Claude model), with its content taken from the claude_message variable.\n",
    "messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]}): After the loop, this line appends one more message to the messages list. This message is from the \"user\" and its content is the last element of the gpt_messages list (gpt_messages[-1]). This seems to be adding the final response from the GPT model as the last user turn in the conversation history for Claude.\n",
    "message = claude.messages.create(...): This line makes a call to the create method of the messages object from the claude library (presumably the Anthropic Claude API library). This is where the function interacts with the Claude language model API.\n",
    "model=claude_model: Specifies the Claude model to use for the completion. The model name is taken from the claude_model variable, which is expected to be defined elsewhere.\n",
    "system=claude_system: Provides a system prompt specifically for the Claude model. The content is taken from the claude_system variable, expected to be defined elsewhere and likely contains instructions for the Claude model.\n",
    "messages=messages: Provides the list of messages for the conversation history that was constructed.\n",
    "max_tokens=500: Sets the maximum number of tokens (words or parts of words) that the Claude model should generate in its response.\n",
    "return message.content[0].text: This line returns the text content of the Claude model's response. The structure of the response from the Claude API is slightly different from the OpenAI API. message.content is a list of content blocks, and [0].text accesses the text content of the first block.\n",
    "In summary, the call_claude function constructs a conversation history for the Claude model by interleaving previous messages from gpt_messages and claude_messages, with the GPT messages often treated as user input for Claude. It includes a specific system prompt for Claude, sends this history to a specified Claude model using the claude library, and returns the generated text response from the Claude model. This function seems designed for a scenario where you are managing a conversation that involves both GPT and Claude models, potentially having them respond to each other's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448fbcbb-9394-4a3f-a3e1-e01f5842010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code snippet sets up and runs a simple conversational exchange between what appear to be two different language models, referred to as \"GPT\" and \"Claude\".\n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "gpt_messages = [\"Hi there\"]: Initializes a list called gpt_messages with an initial greeting from the \"GPT\" side of the conversation.\n",
    "claude_messages = [\"Hi\"]: Initializes a list called claude_messages with an initial greeting from the \"Claude\" side.\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\"): Prints the initial message from GPT.\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\"): Prints the initial message from Claude.\n",
    "for i in range(5):: This loop runs the conversational exchange 5 times.\n",
    "gpt_next = call_gpt(): Inside the loop, this line calls the call_gpt() function (defined in a previous cell) to get the next response from the GPT model. The generated response is stored in the gpt_next variable.\n",
    "print(f\"GPT:\\n{gpt_next}\\n\"): Prints the new response from GPT.\n",
    "gpt_messages.append(gpt_next): Appends the new GPT response to the gpt_messages list, adding it to the conversation history for GPT.\n",
    "claude_next = call_claude(): Calls the call_claude() function (defined in a previous cell) to get the next response from the Claude model. The generated response is stored in the claude_next variable.\n",
    "print(f\"Claude:\\n{claude_next}\\n\"): Prints the new response from Claude.\n",
    "claude_messages.append(claude_next): Appends the new Claude response to the claude_messages list, adding it to the conversation history for Claude.\n",
    "In essence, this code simulates a back-and-forth conversation for 5 turns, where each model's response is generated based on the ongoing conversation history with both models, and the turns are printed to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
