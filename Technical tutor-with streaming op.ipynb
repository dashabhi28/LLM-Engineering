{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "Technical Tutor \n",
    "A tool that takes a technical question,  and responds with an explanation. It is assumed that a local llm is running on your system and has been invoked with this code, for instance here llama 3.2 is engaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question and prompts\n",
    "question = \"\"\"Please explain what this code does and why: yield from {book.get(\"author\") for book in books if book.get(\"author\")} \"\"\"   # here is the question; type over this to ask something new\n",
    "system_prompt = \"You are a technical assistant that analyzes the question raised by the user and provides a specific response\"\n",
    "user_prompt = \"Please give a detailed, short and precise, explanation to the following: \" + question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110aa0f-0d70-4569-a56f-2efd70f7356f",
   "metadata": {},
   "source": [
    "This code snippet creates a Python list of dictionaries called messages. This structure is commonly used to represent a conversation history when interacting with language models.\n",
    "\n",
    "messages = [...]: This initializes a list named messages.\n",
    "{\"role\": \"system\", \"content\": system_prompt}: This is the first dictionary in the list.\n",
    "\"role\": \"system\": Indicates that this message is from the \"system\". System messages are often used to provide instructions or context to the language model about how it should behave.\n",
    "\"content\": system_prompt: The content of this message is the value of the variable system_prompt. This variable is likely defined elsewhere in your notebook and contains the specific instructions for the language model.\n",
    "{\"role\": \"user\", \"content\": user_prompt}: This is the second dictionary in the list.\n",
    "\"role\": \"user\": Indicates that this message is from the \"user\", representing the user's input or query.\n",
    "\"content\": user_prompt: The content of this message is the value of the variable user_prompt. This variable is likely defined elsewhere and contains the user's actual question or request to the language model.\n",
    "In summary, this code is preparing a simple two-turn conversation where the first message sets the stage (with instructions from the system_prompt) and the second message is the user's input (from the user_prompt). This messages list would then typically be passed to a language model API to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc3b58e-1434-420a-ba5f-a344ce5877a1",
   "metadata": {},
   "source": [
    "This code snippet makes a streaming API call to a language model and displays the response as it's being generated.\n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "stream = openai.chat.completions.create(...): This line initiates a chat completion request to the API using the openai library.\n",
    "model=\"llama3.2\": Specifies the language model to use (llama3.2).\n",
    "messages=messages: Provides the list of messages for the conversation history. The messages variable is expected to be a list of dictionaries in the format we've discussed previously ({\"role\": \"...\", \"content\": \"...\"}).\n",
    "stream=True: This is the crucial part. It tells the API to stream the response back in chunks as it's being generated, rather than waiting for the entire response to be ready.\n",
    "response = \"\": Initializes an empty string variable response to store the accumulating streamed content.\n",
    "display_handle = display(Markdown(\"\"), display_id=True): This line sets up the initial display area in the notebook output.\n",
    "display(Markdown(\"\")): Displays an empty Markdown cell initially.\n",
    "display_id=True: Assigns a unique display ID to this output area. This ID is important because it allows you to update this specific output area later.\n",
    "for chunk in stream:: This loop iterates over the chunks of the response as they are streamed from the API.\n",
    "response += chunk.choices[0].delta.content or '': Inside the loop, for each chunk received:\n",
    "chunk.choices[0].delta.content: Accesses the content of the current chunk. delta.content contains the new piece of text generated by the model in this chunk.\n",
    "or '': This handles cases where delta.content might be None (e.g., at the beginning or end of the stream), ensuring that an empty string is added instead.\n",
    "response += ...: Appends the content of the current chunk to the response string.\n",
    "response = response.replace(\"```\",\"\").replace(\"markdown\", \"\"): This line cleans up the response by removing triple backticks (```) and the word \"markdown\". This is often done when the model outputs code blocks formatted with \n",
    "Markdown, and you want to display the raw text without the formatting.\n",
    "update_display(Markdown(response), display_id=display_handle.display_id): This line updates the display area with the accumulating response content.\n",
    "Markdown(response): Creates a Markdown object from the current response string.\n",
    "display_id=display_handle.display_id: Specifies the display area to update using the ID obtained earlier.\n",
    "In essence, this code sets up a streaming connection to a language model, continuously receives parts of the generated response, builds the complete response string chunk by chunk, and dynamically updates a single output area in the notebook to show the response as it's being generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Code Explanation**\n",
       "\n",
       "python\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "\n",
       "\n",
       "This line of code generates a sequence (e.g., iterator or list) containing the values of `book`'s `\"author\"` key for each book object where the `\"author\"` key exists.\n",
       "\n",
       "**Breakdown:**\n",
       "\n",
       "- `(book.get(\"author\"))`: This part uses the `get()` method to retrieve the value associated with the `\"author\"` key from each book dictionary. If the key does not exist, it will return `None`.\n",
       "\n",
       "- `{... for book in books if book.get(\"author\")}`: This is a \"dictionary comprehension\" or \"set comprehension\", which generates a new set (in Python 3) containing values that match the condition. In this case, it only includes dictionaries (`book`) where `\"author\"` exists.\n",
       "\n",
       "- `yield from`: This keyword sends each element in the generator expression to `yield from ...` instead of generating it first in memory, effectively creating an iterator.\n",
       "\n",
       "**Example Use Case**\n",
       "\n",
       "python\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book 2\", \"author\": \"\"},\n",
       "    {\"title\": \"Book 3\"}\n",
       "]\n",
       "\n",
       "for author in yield from {book.get(\"author\") for book in books if book.get(\"author\") if book.get(\"author\") is not None}:\n",
       "    print(author)\n",
       "\n",
       "\n",
       "In this example, the code will only iterate over authors that exist (`Author A`), skipping authors with missing `\"author\"` values."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Calling the model and initiating response\n",
    "stream = openai.chat.completions.create(model=\"llama3.2\", messages=messages,stream=True)\n",
    "    \n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    response += chunk.choices[0].delta.content or ''\n",
    "    response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642a014-abef-47a2-b6be-13f2b106151a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
